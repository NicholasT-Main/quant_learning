{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Automated Correlation-Based Stock Classification with Machine Learning Forecasting\n",
    "\n",
    "### Project Summary\n",
    "\n",
    "A comprehensive Python-based system that automatically classifies stocks into correlation-based groups and applies machine learning models for price forecasting. The project incorporated extensive research and testing of multiple correlation methodologies including Pearson, Spearman, EWMA, Rolling Window, Kendall Tau, Partial Correlation, Distance Correlation, Mutual Information, and Copula-Based correlation to determine the optimal approach for financial time series analysis.\n",
    "\n",
    "Through systematic evaluation, EWMA (Exponentially Weighted Moving Average) correlation was selected as the primary methodology due to its superior ability to weight recent market data more heavily while maintaining computational efficiency in volatile market conditions.\n",
    "\n",
    "The platform processes 200+ stock tickers with complete daily historical data using the yfinance API, with scalable architecture designed to handle significantly larger datasets. The correlation engine dynamically groups stocks based on configurable minimum correlation thresholds, enabling flexible analysis across different market conditions and sectors.\n",
    "\n",
    "Following comprehensive research and comparative testing of machine learning approaches including Neural Networks, Long Short-Term Memory (LSTM) networks, Recurrent Neural Networks (RNN), Random Forest, Support Vector Machines (SVM), Gradient Boosting methods, and Time Series Transformers, the system implements the optimal forecasting model selected based on performance metrics.\n",
    "\n",
    "Through systematic evaluation, _______ was selected as the primary forecasting methodology due to its superior performance in [specific reasons to be filled based on testing results].\n",
    "\n",
    "Built with a robust technology stack including Python, pandas, numpy, scikit-learn, PyTorch, and matplotlib for data processing, statistical analysis, machine learning implementation, and visualization. The system emphasizes statistical rigor and quantitative analysis while eliminating human bias from stock classification and prediction processes. Comprehensive performance metrics and backtesting capabilities validate model effectiveness across various market conditions.\n",
    "\n",
    "**Status:** In active development with correlation analysis complete and ML forecasting module in progress.<br>\n",
    "**Author:** Nicholas Taylor <br>\n",
    "**Date:** 2025-06"
   ],
   "id": "46d0bff853e9baca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## imports\n",
    "\n"
   ],
   "id": "411a23958f80c439"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Utilities import machine_learning as ml\n",
    "from Utilities import ticker_manipulation as tm\n",
    "from Utilities import grouping\n",
    "from Utilities import visualisations as vis\n",
    "from Utilities import timing\n",
    "import matplotlib.pyplot as plt\n",
    "from win32api import mouse_event\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import csv\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##",
   "id": "ee0854ef6bb134da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ticker_list = ['KO', 'MDB', 'BAC', 'BLK', 'SYK', 'MRK', 'AAL', 'PINS', 'LOW', 'DAL', 'SHW', 'LLY', 'ASML', 'UNH', 'PSX', 'CRM', 'HSY', 'AFRM', 'CHTR', 'PCAR', 'DECK', 'SPGI', 'RHHBY', 'SHOP', 'WFC', 'CVX', 'ABBV', 'BMY', 'AEP', 'PYPL', 'JNJ', 'AXP', 'NVS', 'YUM', 'MSCI', 'ECL', 'MDLZ', 'HCA', 'SBUX', 'UAL', 'CHD', 'KMI', 'XOM', 'CVS', 'GILD', 'HLT', 'QSR', 'LHX', 'GOOGL', 'SLB', 'DDOG', 'VWAGY', 'EQNR', 'AMD', 'VALE', 'COST', 'AMAT', 'GE', 'WMT', 'NVDA', 'NOW', 'TCEHY', 'D', 'COP', 'JD', 'ELV', 'IQV', 'CBOE', 'RIO', 'SAP', 'SCHW', 'LYV', 'TDOC', 'NEE', 'SO', 'ROST', 'MKC', 'STLD', 'CRWD', 'MS', 'DVN', 'PCG', 'HON', 'APD', 'GS', 'REGN', 'KHC', 'INFY', 'BIDU', 'SRE', 'TTE', 'CSCO', 'NET', 'CMCSA', 'ADDYY', 'MAR', 'NFLX', 'SNOW', 'CZR', 'APA', 'GM', 'SNAP', 'NDAQ', 'META', 'NUE', 'EW', 'VLO', 'VRTX', 'HES', 'AZN', 'MSFT', 'BA', 'ADBE', 'SPOT', 'V', 'BHP', 'CCL', 'BSX', 'NTES', 'BKNG', 'TSLA', 'LMT', 'ZS', 'K', 'F', 'TJX', 'SKX', 'T', 'SEDG', 'BEP', 'SPWR', 'DOW', 'NOC', 'IBM', 'UL', 'BYD', 'LCID', 'TM', 'ETN', 'LVMUY', 'MCD', 'WYNN', 'CLX', 'C', 'PENN', 'DUK', 'FI', 'BABA', 'WBD', 'AMGN', 'ENPH', 'ADSK', 'EL', 'AVGO', 'RCL', 'MA', 'DG', 'SHEL', 'LIN', 'X', 'QCOM', 'TMO', 'PEG', 'MPC', 'OXY', 'EMR', 'DD', 'FCX', 'HD', 'CL', 'RTX', 'PPG', 'GPN', 'MDT', 'INTU', 'GD', 'FANG', 'ETSY', 'AAPL', 'WMB', 'PM', 'ROKU', 'INTC', 'ZBH', 'TSM', 'CAT', 'PFE', 'LRCX', 'VZ', 'RIVN', 'MGM', 'ACN', 'DOV', 'HRL', 'CMI', 'MU', 'KLAC', 'PARA', 'DPZ', 'KMB', 'PLTR', 'DE', 'ICE', 'BP', 'AMZN', 'DLTR', 'AA', 'PG', 'FSLR', 'ABNB', 'OKTA', 'GIS', 'TDG', 'EIX', 'UBER', 'DOCU', 'EXC', 'DIS', 'UAA', 'TGT', 'WIT', 'ISRG', 'ED', 'HAL', 'PEP', 'WDAY', 'RUN', 'EPAM', 'BKR', 'LYFT', 'TMUS', 'PDD', 'NKE', 'ORCL', 'NEM', 'JPM', 'LULU', 'PANW', 'MO', 'EOG', 'FTNT', 'MRNA', 'IR', 'CME', 'ULTA', 'LYB', 'EQIX']\n",
    "\n",
    "test_tickers=['ASML', 'AMAT', 'LRCX', 'KLAC','COP', 'DVN', 'APA', 'FANG', 'EOG']"
   ],
   "id": "b56398a4d3e3928a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Config\n",
    "*start_date* is the day it starts tracking stock values from <br>\n",
    "*end_date* is the day it ends tracking stock values on <br>\n",
    "\n",
    "*ewma_span* is the estimated waited mean value which is the decay of the  weighted mean value higher  value is a lower decay<br>\n",
    "(Common values: 20 (approx. 1 month), 60 (approx. 3 months))\n",
    "\n",
    "*grouping_minimum_correlation* is the correlation value required to group 2 stocks <br>"
   ],
   "id": "1b28a272392d5656"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Config\n",
    "\n",
    "#list dates to map for\n",
    "start_date = \"2020-01-01\"\n",
    "#end_date = \"2025-07-12\"\n",
    "end_date=dt.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "# --- EWMA Configuration ---\n",
    "# 'span' determines the decay. A smaller span means faster decay and more weight to recent data.\n",
    "# Common values: 20 (approx. 1 month), 60 (approx. 3 months).\n",
    "ewma_span = 720 # You can adjust this value based on how much you want to prioritize recent data\n",
    "\n",
    "#minimum correlation required for any value to be added to a group\n",
    "#can be between it and any value in the group, does not require all values\n",
    "grouping_minimum_correlation=0.80\n"
   ],
   "id": "31f4bc3bca43d072",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data retrieval\n",
    "This function fetches all the required data from the yfinance API and writes it to the all_dict dictionary, storing the stock tickers as keys and the stock close prices as values\n",
    "We are using the timer_function to measure time taken for scalability"
   ],
   "id": "6675c2d5b77e58da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "time,all_dict=timing.timer_function(lambda: tm.get_tickers_data(test_tickers,start_date,end_date))\n",
    "print(f\"time taken:{time}\")"
   ],
   "id": "47300f2141d0cef7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Visualisation\n",
    "This function plots the close prices of each ticker on a matplotlib graph <br>\n",
    "_function is found in visualisations.py_\n"
   ],
   "id": "c849db1920aac2bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "vis.plot_tickers_values(all_dict)",
   "id": "5c654c3b29eda745",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This function plots the returns calculated from close prices of each ticker on a matplotlib graph <br>\n",
    "_function is found in visualisations.py_"
   ],
   "id": "1819f5711f346b06"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_returns=vis.plot_tickers_return(all_dict)",
   "id": "c1d05eb5f36586e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exploring Correlation Methods\n",
    "\n",
    "### Pearson Correlation\n",
    "Linear correlation that measures the strength and direction of a linear relationship between two variables.<br>\n",
    "**Use in stocks:** Not ideal on its own; it gives equal weight to all data points, so it doesnâ€™t capture recent dynamics or handle outliers well.\n",
    "\n",
    "---\n",
    "### Spearman Correlation\n",
    "Rank-based correlation that assesses how well the relationship between two variables can be described using a monotonic function.<br>\n",
    "**Use in stocks:** More robust to outliers and useful for capturing consistent directional relationships, even if not linear.\n",
    "\n",
    "---\n",
    "### EWMA Correlation\n",
    "Exponentially Weighted Moving Average correlation that gives more weight to recent data points.<br>\n",
    "**Use in stocks:** Very useful for tracking changing relationships over time, especially in volatile markets.\n",
    "\n",
    "---\n",
    "### Rolling Window Correlation\n",
    "Computes correlation over a sliding window of time, capturing localized relationships.<br>\n",
    "**Use in stocks:** Good for observing short-term changes in correlation, particularly useful in backtesting strategies.\n",
    "\n",
    "---\n",
    "### Additional Methods Explored\n",
    "\n",
    "**Kendall Tau Correlation** â€“ Rank-based correlation like Spearman but with a different calculation; slower and more niche, useful for small datasets.<br>\n",
    "**Partial Correlation** â€“ Measures the relationship between two variables while controlling for others; useful when filtering out confounding effects.<br>\n",
    "**Distance Correlation** â€“ Captures both linear and nonlinear relationships; detects hidden dependencies but not limited to -1 to 1 range.<br>\n",
    "**Mutual Information** â€“ Measures how much information one variable gives about another; useful in machine learning and nonlinear analysis.<br>\n",
    "**Copula-Based Correlation** â€“ Captures dependency structures in the tails of distributions; valuable for modeling extreme events and risk.\n",
    "\n",
    "-------------------------------------------------------------------------------------------\n",
    "### Choice: EWMA Correlation\n",
    "The choice of correlation method is the Exponentially Weighted Moving Average, this has been chosen as we are working with time series data where more recent values have a larger affect then older values on the market. EWMA Correlation is less computationally expensive as it follows linear time complexity, O(n) due to a single pass through the data, and requires less memory than Rolling Window Correlation as it only requires storing the most recent correlation value."
   ],
   "id": "890e78d74de1fd88"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Calculate Exponentially Weighted Moving Average (EWMA) Correlation Matrix ---\n",
    "# This gives more weight to recent values.\n",
    "# The `span` parameter determines the decay rate.\n",
    "# .corr() on the EWM object computes the EWMA correlation matrix.\n",
    "# We then take the last complete correlation matrix from the multi-indexed result.\n",
    "time, ewma_corr_df = timing.timer_function(lambda: (df_returns.ewm(span=ewma_span).corr()))\n",
    "\n",
    "# Get the most recent EWMA correlation matrix\n",
    "# df_returns.index[-1] gets the last date in your returns DataFrame.\n",
    "correlation_matrix = ewma_corr_df.xs(df_returns.index[-1], level=0)\n",
    "print(f\"time taken: {time}\")"
   ],
   "id": "fe424fdaa2c8fcbc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Visualising correlation using a heatmap\n",
    "This function plots a heatmap comparing the correlation of all the given stock tickers individually. <br>\n",
    "_function is found in visualisations.py_"
   ],
   "id": "1c59f0d8a0972079"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "vis.heatmap_correlations(correlation_matrix)",
   "id": "7a39bacb6b724ca4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exploring Grouping Methods\n",
    "\n",
    "### All Grouping\n",
    "\n",
    "Groups stocks where every stock correlates with every other stock in the group above the minimum threshold.\n",
    "\n",
    "- **Use in stocks:** Creates tight, high-quality clusters; ideal for identifying stocks that move together consistently.\n",
    "- **Complexity:** O(nÂ³) theoretical, but often faster in practice\n",
    "- **Best for:** High-quality sector clusters, risk management, pair trading\n",
    "\n",
    "---\n",
    "\n",
    "### Any Grouping\n",
    "\n",
    "Groups stocks where each stock correlates with at least one other stock in the group above the minimum threshold.\n",
    "\n",
    "- **Use in stocks:** Fast initial grouping for broad sector identification\n",
    "- **Complexity:** O(nÂ² Ã— g) where g is average group size\n",
    "- **Best for:** Initial broad categorization, sector discovery\n",
    "\n",
    "---\n",
    "\n",
    "### Additional Grouping Methods\n",
    "\n",
    "- **Hierarchical Clustering** â€“ Bottom-up, slower but more precise\n",
    "- **HUB Correlation** â€“ main stock with correlations, can be inaccurate.\n",
    "- **K-Means Clustering** â€“ Partitioning-based; requires number of clusters\n",
    "- **DBSCAN Clustering** â€“ Density-based; finds outliers\n",
    "- **Graph-Based Clustering** â€“ Uses correlation as edges in a graph\n",
    "- **Dynamic Time Warping** â€“ Groups by shape similarity; great for time-lagged trends\n",
    "\n",
    "___\n",
    "### Choice: ALL Correlation Grouping\n",
    "The choice of grouping method is ALL Correlation, selected for its superior performance characteristics: <br>\n",
    "**Quality over quantity**: Creates tighter, more reliable groups where every stock genuinely correlates with every other stock, ensuring group coherence. <br>\n",
    "**Unexpected performance advantage**: Despite O(nÂ³) complexity, proves faster in practice due to early elimination of unsuitable stocks and smaller resulting group sizes. <br>\n",
    "**Risk management benefits**: High-quality groups are more reliable for portfolio construction and risk assessment, as correlation relationships are verified across all pairs. <br>\n",
    "**Computational efficiency**: Early rejection mechanism and smaller groups reduce overall computational overhead compared to methods that create large, loosely-connected groups requiring extensive validation.\n",
    "\n"
   ],
   "id": "c7f2ffd7e326771e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get results from both methods\n",
    "time_any,final_groups_any = timing.timer_function(lambda: grouping.grouping_any_correlation(grouping_minimum_correlation, correlation_matrix, 3))\n",
    "time_all,final_groups_all = timing.timer_function(lambda: grouping.grouping_all_correlate(grouping_minimum_correlation, correlation_matrix, 3))\n",
    "\n",
    "def print_groups_with_names(groups,group_names,time):\n",
    "    print(\"\\n\",group_names,\"(time taken: \", time,\")\")\n",
    "    print(\"=\"*30)\n",
    "    if not groups:\n",
    "        print(\"   No groups found with current parameters.\\n\")\n",
    "        return\n",
    "\n",
    "    for i, group in enumerate(groups, 1):\n",
    "        print(f\"\\n Stock Group {i} ({len(group)} stocks):\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        for ticker in group:\n",
    "            try:\n",
    "                company_name = yf.Ticker(ticker).info.get('longName', 'Name not available')\n",
    "                print(f\"   {ticker:6} = {company_name}\")\n",
    "            except:\n",
    "                print(f\"   {ticker:6} = [Error retrieving name]\")\n",
    "        print()\n",
    "\n",
    "# Display results for each method\n",
    "print_groups_with_names(final_groups_any, \"ANY Correlation\", time_any)\n",
    "print_groups_with_names(final_groups_all, \"ALL Correlate\",time_all)\n",
    "\n",
    "final_groups = final_groups_all"
   ],
   "id": "ed234e07ee12cbf8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Plotting groups:\n",
    "this code plots all the automatically created groups in order to visualise the groups, this includes a group average price line which is the for each group which is an average return of the group.\n",
    "This visualisation is not intuitable as we are plotting the group based on their value returns, yet grouping based on there percentage returns, this is showing how the stocks move in similar ways.\n",
    "The text following each image is the correlation of each stock against the average of the rest of the stocks this a measure of how similarly the stocks move together, the final line is the intergroup correlation which is the correlation of the all the stocks in the group."
   ],
   "id": "b4677cd7ff9d3cf0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for group in final_groups:\n",
    "    #Plot individual and average prices\n",
    "    group_closes = vis.plot_group_prices_with_avg(group, all_dict)\n",
    "\n",
    "    #EWMA-based correlation\n",
    "    vis.print_ewma_correlations(group, group_closes, ewma_span)\n",
    "\n",
    "    #Pairwise correlation from precomputed matrix\n",
    "    group_corr_matrix = correlation_matrix.loc[group, group]\n",
    "\n",
    "    avg_corr = group_corr_matrix.where(np.triu(np.ones_like(group_corr_matrix, dtype=bool), k=1)).stack().mean()\n",
    "    print(f\"intergroup correlation: {avg_corr:.5f}\")"
   ],
   "id": "1bd508f6d8410a10",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "vis.plot_stock_distributions(final_groups_all,df_returns)",
   "id": "da5bb637b97d8655",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## What is available:\n",
    "- **_final_groups_**: list of lists of stocks in correlated groups\n",
    "- **_all_dict_**: dictionary of all the stocks values, with their tickers as keys\n",
    "- **_df_returns_**: dictionary of all the return percentage values, with their tickers as keys"
   ],
   "id": "e6f2b76a2f51573a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(final_groups)\n",
    "#print(all_dict.keys())\n",
    "#print(df_returns.keys())"
   ],
   "id": "66feb2306a7b01dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Machine Learning Testing\n",
    "## Testing Method\n",
    "---\n",
    "We will use a walk forward approach to testing these different prediction models to test real world accuracy for these predictions\n",
    "\n"
   ],
   "id": "ac00dae8e2351e23"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "stop # this is to not run the ml while testing other featurestures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor  # Example model - replace with your actual model\n",
    "\n",
    "for group in final_groups:\n",
    "    # 1. Build feature matrix using all stocks in the group\n",
    "    X = pd.concat([df_returns[ticker] for ticker in group], axis=1)\n",
    "    X.columns = group  # Label columns with ticker names\n",
    "\n",
    "    # 2. Get closing prices for the group to calculate average return\n",
    "    group_returns = pd.DataFrame({ticker: df_returns[ticker] for ticker in group})\n",
    "\n",
    "    # 3. Calculate group average return and shift to get next-day return\n",
    "    y = group_returns.mean(axis=1).shift(-1)\n",
    "\n",
    "    # 4. Drop rows with NaNs (from pct_change and shift)\n",
    "    valid_idx = (~X.isnull().any(axis=1)) & (~y.isnull())\n",
    "    X = X.loc[valid_idx]\n",
    "    y = y.loc[valid_idx]\n",
    "\n",
    "    # 5. Initialize your model\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "    # 6. Create walk-forward predictor\n",
    "    wf_predictor = ml.WalkForwardPredictor(\n",
    "        model=model,\n",
    "        min_train_size=10,\n",
    "        retrain_frequency=1\n",
    "    )\n",
    "\n",
    "    # 7. Run walk-forward prediction\n",
    "    print(f\"\\nStarting walk-forward analysis for group: {', '.join(group)}\")\n",
    "    predictions, actuals, dates = wf_predictor.predict_and_learn(X, y)\n",
    "\n",
    "    # 8. Plot results using one sample stock just for visual reference\n",
    "    ml.plot_walkforward_results(wf_predictor, all_dict[group[0]]['Close'], target_stock=\"avg for group\")\n",
    "\n",
    "    # 9. Analyze performance\n",
    "    performance_metrics = ml.analyze_walkforward_performance(wf_predictor)\n",
    "\n",
    "    # 10. Create detailed comparison table\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Date': wf_predictor.prediction_dates,\n",
    "        'Actual_Return': wf_predictor.actual_values,\n",
    "        'Predicted_Return': wf_predictor.predictions,\n",
    "        'Error': wf_predictor.errors,\n",
    "        'Abs_Error': np.abs(wf_predictor.errors)\n",
    "    })\n",
    "\n",
    "    #print(\"\\nFirst 10 predictions:\")\n",
    "    #print(comparison_df.head(10).round(6))\n",
    "    #print(\"\\nLast 10 predictions:\")\n",
    "    #print(comparison_df.tail(10).round(6))"
   ],
   "id": "2005a3408ea1646",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3748b0d7da78fac7",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
